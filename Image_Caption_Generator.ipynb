{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Caption Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCfsghBbkA2_",
        "colab_type": "code",
        "outputId": "24084640-67ed-45a5-942c-54498b44f56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD7P5AuYQwuL",
        "colab_type": "code",
        "outputId": "1fc8edf9-09f1-4daa-d2c9-ba1f21620707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My Drive/Machine Learning/IMAGE CAPTIONING"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Machine Learning/IMAGE CAPTIONING\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyGQagjv9aQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!kaggle datasets download -d ming666/flicker8k-dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "os5KbNKqW7iV",
        "outputId": "ecad508d-dc48-4254-bd4c-062ca3cb33d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from pickle import dump, load\n",
        "import pandas as pd \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv1VApLfDeEX",
        "colab_type": "text"
      },
      "source": [
        "# FILE PATHS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYVxPlmaDGpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image = \"flickr8k_dataset/Flicker8k_Dataset/\"\n",
        "Train = \"flickr8k_text/Flickr_8k.trainImages.txt\"\n",
        "Test =  \"flickr8k_text/Flickr_8k.testImages.txt\"\n",
        "text = \"flickr8k_text/Flickr8k.lemma.token.txt\"\n",
        "Dev = \"flickr8k_text/Flickr_8k.devImages.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7jmZyLoD_7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-KUYrnkJPNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Imagepath = os.path.join(path , Image)\n",
        "TrainPATH = os.path.join(path, Train)\n",
        "TestPATH = os.path.join(path,Test)\n",
        "textPATH = os.path.join(path, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_t8s_xUJqV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_Doc(path):\n",
        "    \n",
        "    file = open(path, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_Desc(doc):\n",
        "    \n",
        "    mapping= dict()\n",
        "    for line in doc.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        if len(line)<2:\n",
        "            continue\n",
        "        #get imageid\n",
        "        imageid = tokens[0].split('.')[0]\n",
        "        #get description\n",
        "        desc = \" \".join(tokens[1:])\n",
        "        #check if imageid exist\n",
        "        if imageid not in mapping:\n",
        "            mapping[imageid]= list()\n",
        "        mapping[imageid].append(desc)\n",
        "    return mapping\n",
        "\n",
        "\n",
        "def load_identifiers(path):\n",
        "    \n",
        "    doc = load_Doc(path)\n",
        "    dataset = list()\n",
        "    for line in doc.split(\"\\n\"):\n",
        "        if len(line)<1:\n",
        "            continue\n",
        "        identifiers = line.split('.')[0]\n",
        "        dataset.append(identifiers)\n",
        "    return set(dataset)\n",
        "\n",
        "\n",
        "def cleaning(description):\n",
        "    # prepare translation table for removing punctuations\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    for imageid, desc_list in description.items():\n",
        "        for i in range(len(desc_list)):\n",
        "            desc = desc_list[i]\n",
        "            desc = desc.split()\n",
        "            desc = [word.lower() for word in desc]\n",
        "            # remove punctuation from each token\n",
        "            desc = [w.translate(table) for w in desc]\n",
        "            # remove hanging 's' and 'a'\n",
        "            desc = [word for word in desc if len(word)>1]\n",
        "            # remove tokens with numbers in them\n",
        "            desc = [word for word in desc if word.isalpha()]\n",
        "            # store as string\n",
        "            desc_list[i] =  ' '.join(desc)\n",
        "\n",
        "\n",
        "def load_clean_desc(filename, dataset):\n",
        "    \n",
        "    doc = load_Doc(filename)\n",
        "    descriptions = dict()\n",
        "    for line in doc.split(\"\\n\"):\n",
        "        tokens = line.split()\n",
        "        imageid, imagedesc = tokens[0], tokens[1:]\n",
        "        if imageid in dataset:\n",
        "        #skip image if not in dataset\n",
        "            if imageid not in descriptions:\n",
        "                descriptions[imageid] = list()\n",
        "            desc = 'startseq' + ' '.join(imagedesc) + 'stopseq'\n",
        "            descriptions[imageid].append(desc)\n",
        "    return descriptions\n",
        "\n",
        "\n",
        "\n",
        "def to_vocabolary(desc):\n",
        "    \n",
        "    vocab_set = set()\n",
        "    for key in desc.keys():\n",
        "        [vocab_set.update(d) for d in desc[key]]\n",
        "    return vocab_set\n",
        "\n",
        "\n",
        "def save_description(desc, filename):\n",
        "    lines = list()\n",
        "    for keys, des in desc.items():\n",
        "        for d in des:\n",
        "            lines.append(keys+' '+d)\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "\n",
        "def to_line(descriptions):\n",
        "    lines =[]\n",
        "    for keys in descriptions.keys():\n",
        "        [lines.append(d) for d in descriptions[keys]]\n",
        "    return lines\n",
        "\n",
        "def load_tokens(description):\n",
        "    desc = to_line(description)\n",
        "    token = Tokenizer()\n",
        "    token.fit_on_texts(desc)\n",
        "    return token\n",
        "\n",
        "\n",
        "\n",
        "def max_length(description):\n",
        "    lines = to_line(description)\n",
        "    return max(len(d.split()) for d in lines)\n",
        "\n",
        "\n",
        "def create_sequence(tokeniser, maxlength, photo, description, vocab_size):\n",
        "    X1,X2,Y = list(),list(),list()\n",
        "    \n",
        "    # walk through each image identifier\n",
        "    for key, desc_list in description.items():\n",
        "        for d in desc_list:\n",
        "            seq = tokeniser.texts_to_sequences([d])[0]\n",
        "            #split each seq into X,Y pair\n",
        "            for i in range(1,len(seq)):\n",
        "                input_seq, out_seq = seq[:i],seq[i]\n",
        "                #pad the input sequence\n",
        "                in_seq = pad_sequences([input_seq], maxlen=max_length)[0]\n",
        "                #encode the output sequence\n",
        "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                #store all values\n",
        "                X1.append(photo[key][0])\n",
        "                X2.append(in_seq)\n",
        "                Y.append(out_seq)\n",
        "        \n",
        "    return np.array(X1),np.array(X2),np.array(Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getmodel():\n",
        "    #load model\n",
        "    model = VGG16()\n",
        "    #remove output layer\n",
        "    model.layers.pop()\n",
        "    model = Model(inputs= model.inputs , outputs= model.layers[-2].output)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_features(directory):\n",
        "    \n",
        "    #feature dict\n",
        "    features = dict()\n",
        "    print(\"[INFO] loading model.....\")\n",
        "    model = getmodel()\n",
        "    print(\"[INFO] Model loaded.....\")\n",
        "    for name in tqdm(os.listdir(directory)):\n",
        "        \n",
        "        image_id = name.split('.')[0]\n",
        "        #get filename\n",
        "        filename = os.path.join(directory, name)\n",
        "        #load image\n",
        "        image = load_img(filename, target_size=(224,224))\n",
        "        #convert to array\n",
        "        image = img_to_array(image)\n",
        "        #reshape image to input size to the model\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        #prepare image for VGG model\n",
        "        image = preprocess_input(image)\n",
        "        \n",
        "        #get features\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        #store features\n",
        "        features[image_id] = feature\n",
        "        \n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def dumpfile(output, file):\n",
        "    return dump(file, open(output, 'wb'))\n",
        "\n",
        "\n",
        "def load_photo_features(filename, dataset):\n",
        "    #load features from pickle file\n",
        "    feat = load(open(filename, 'rb'))\n",
        "    features = {k:feat[k] for k in dataset}\n",
        "    return features\n",
        "\n",
        "\n",
        "def define_model(vocab_size, max_length):\n",
        "    \n",
        "    #encoder1\n",
        "    input1 = Input(shape=(4096,), name='Encoder1')\n",
        "    en1    = Dropout(0.5)(input1)\n",
        "    en2    = Dense(256,activation='relu' )(en1)\n",
        "    \n",
        "    #encoder2\n",
        "    input2 = Input(shape=(max_length,), name='Encoder2')\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(input2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "    \n",
        "    #decoder\n",
        "    merge = add([en2,se3])\n",
        "    decoder1 = Dense(256, activation=\"relu\")(merge)\n",
        "    output = Dense(vocab_size, activation='softmax')(decoder1)\n",
        "    \n",
        "    #put it together\n",
        "    model = Model(inputs=[input1, input2], outputs=output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42yGGSG0LH3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = load_Doc(textPATH)\n",
        "#load descriptions from textfile\n",
        "description = load_Desc(doc)\n",
        "#clean the descriptions\n",
        "cleaning(description)\n",
        "\n",
        "#save the description\n",
        "save_description(description, \"description.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y7XMgwEMPJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import load\n",
        "features = load(open('features.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycmAk6mISjzJ",
        "colab_type": "code",
        "outputId": "0ebd419f-84f6-4ea3-b620-7c0147eefe71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#Load Training data\n",
        "train = load_identifiers(TrainPATH)\n",
        "print('Dataset Train: ', len(train))\n",
        "\n",
        "\"=====================================================================\"\n",
        "\n",
        "#load train set description\n",
        "train_descriptions = load_clean_desc('description.txt', train)\n",
        "print(\"Descriptions Train: \", len(train_descriptions))\n",
        "\n",
        "\"======================================================================\"\n",
        "\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print(\"train features:\", len(train_features))\n",
        "\n",
        "\"=======================================================================\"\n",
        "\n",
        "\n",
        "# Get tokens \n",
        "tokens = load_tokens(train_descriptions)\n",
        "vocab  = len(tokens.word_index)+1\n",
        "print('Vocab Size:', vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Train:  6000\n",
            "Descriptions Train:  6000\n",
            "train features: 6000\n",
            "Vocab Size: 8152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRkVFJspSqfs",
        "colab_type": "code",
        "outputId": "0a561097-d01a-48cb-eca2-07238c217a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length:', max_length)\n",
        "# prepare sequences\n",
        "X1train, X2train, ytrain = create_sequence(tokens, max_length,features,train_descriptions, vocab)\n",
        "print('Size of sequence',len(X2train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Description Length: 32\n",
            "Size of sequence 240907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9fAMyNVUEz2",
        "colab_type": "code",
        "outputId": "a8f80a6c-8c11-4cc0-a42f-7c9da7180bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "test = load_identifiers(Dev)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_desc('description.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features('features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "# prepare sequences\n",
        "X1test, X2test, ytest = create_sequence(tokens, max_length, test_features,test_descriptions, vocab)\n",
        "print('Size of text Sequence:', len(X2test))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n",
            "Size of text Sequence: 39571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR4xCUe3S-8B",
        "colab_type": "code",
        "outputId": "37164084-82e4-486d-c23f-a80ab09943b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "# define the model\n",
        "model = define_model(vocab, max_length)\n",
        "# define checkpoint callback\n",
        "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Encoder2 (InputLayer)           [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder1 (InputLayer)           [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 32, 256)      2086912     Encoder2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           Encoder1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 8152)         2095064     dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,821,912\n",
            "Trainable params: 5,821,912\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hQMJAqxU7qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # fit model\n",
        "#model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOr_QdBrlkwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights('model-ep003-loss3.943-val_loss4.169.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8prH2n8lrvi",
        "colab_type": "code",
        "outputId": "1c762e91-f4dc-4a10-e199-33c92424bf40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model.fit([X1train, X2train], ytrain, initial_epoch=3,epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 240907 samples, validate on 39571 samples\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from inf to 4.16759, saving model to model-ep004-loss3.978-val_loss4.168.h5\n",
            "240907/240907 - 727s - loss: 3.9776 - val_loss: 4.1676\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 4.16759\n",
            "240907/240907 - 731s - loss: 3.8289 - val_loss: 4.1842\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 4.16759\n",
            "240907/240907 - 736s - loss: 3.7395 - val_loss: 4.2195\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 4.16759\n",
            "240907/240907 - 736s - loss: 3.6763 - val_loss: 4.2419\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 4.16759\n",
            "240907/240907 - 736s - loss: 3.6265 - val_loss: 4.2916\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 4.16759\n",
            "240907/240907 - 732s - loss: 3.5911 - val_loss: 4.2994\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 4.16759\n",
            "240907/240907 - 731s - loss: 3.5635 - val_loss: 4.3153\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 4.16759\n",
            "240907/240907 - 733s - loss: 3.5457 - val_loss: 4.3936\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 4.16759\n",
            "240907/240907 - 734s - loss: 3.5236 - val_loss: 4.3830\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 4.16759\n",
            "240907/240907 - 729s - loss: 3.5139 - val_loss: 4.4537\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 4.16759\n",
            "240907/240907 - 731s - loss: 3.5009 - val_loss: 4.4718\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 4.16759\n",
            "240907/240907 - 731s - loss: 3.4890 - val_loss: 4.4528\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 4.16759\n",
            "240907/240907 - 730s - loss: 3.4724 - val_loss: 4.4884\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 4.16759\n",
            "240907/240907 - 731s - loss: 3.4695 - val_loss: 4.5299\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 4.16759\n",
            "240907/240907 - 728s - loss: 3.4565 - val_loss: 4.5542\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 4.16759\n",
            "240907/240907 - 727s - loss: 3.4502 - val_loss: 4.5290\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 4.16759\n",
            "240907/240907 - 735s - loss: 3.4498 - val_loss: 4.5569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3787bf5eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-1HTFfC7CUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCzFoS1mXuVY",
        "colab_type": "code",
        "outputId": "eddacd90-4d03-4eb2-db41-bd3c8832a696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "model = load_model('model-ep004-loss3.978-val_loss4.168.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDSYrsLHM-lZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index== integer:\n",
        "      return word\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5MYRZKPNgkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_desc(model, tokenizer, photo, max_length):\n",
        "  in_text ='startseq'\n",
        "\n",
        "  for i in range(max_length):\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    seq = pad_sequences([seq], maxlen=max_length)\n",
        "\n",
        "    yHat = model.predict([photo,seq],verbose=0)\n",
        "    yHat = np.argmax(yHat)\n",
        "\n",
        "    word = word_for_id(yHat, tokenizer)\n",
        "    if word is None:\n",
        "      break\n",
        "    in_text+= \" \" + word\n",
        "    if word == 'endseq':\n",
        "      break\n",
        "  return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwHj-a2XPKTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "  actual, predicted = list(),list()\n",
        "\n",
        "  for key, desc_list in descriptions.items():\n",
        "\n",
        "    yHat = gen_desc(model, tokenizer, photos[key], max_length)\n",
        "\n",
        "    reference = [d.split() for d in desc_list]\n",
        "    actual.append(reference)\n",
        "    predicted.append(yHat.split())\n",
        "  \n",
        "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC5NlXusQYAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thLyTfOkSJ_J",
        "colab_type": "code",
        "outputId": "208b19b7-820a-4ad5-babd-b4e66ede95c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "test = load_identifiers(Test)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_desc('description.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features('features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORuQ-tlgStEO",
        "colab_type": "code",
        "outputId": "08a9a922-b7c8-4b31-b59b-7daf3fcb63ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "evaluate_model(model, test_descriptions, test_features, tokens, max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.166121\n",
            "BLEU-2: 0.083653\n",
            "BLEU-3: 0.059387\n",
            "BLEU-4: 0.021189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGkClFCxTj5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}